{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f17b7e66-5fb9-4db7-8fda-0386ad43174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# # 1. SETUP: IMPORTING LIBRARIES\n",
    "# =============================================================================\n",
    "import torch\n",
    "import cv2 # OpenCV for image handling\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e6b07f-b373-4c75-afa9-1df8a88fdb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\RIYA/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-9-6 Python-3.12.2 torch-2.6.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- YOLOv5 Model Loaded Successfully ---\n",
      "Confidence threshold set to 0.4\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# # 2. LOAD THE PRE-TRAINED YOLOv5 MODEL\n",
    "# =============================================================================\n",
    "# Load the YOLOv5 model from PyTorch Hub. 'yolov5s' is the small, fast version.\n",
    "# 'pretrained=True' downloads the weights trained on the COCO dataset.\n",
    "try:\n",
    "    model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Please ensure you have an active internet connection to download the model.\")\n",
    "    exit()\n",
    "\n",
    "print(\"--- YOLOv5 Model Loaded Successfully ---\")\n",
    "# Set confidence threshold and IOU threshold for filtering detections\n",
    "model.conf = 0.4  # Confidence threshold (0-1)\n",
    "model.iou = 0.45  # Intersection over Union threshold (0-1)\n",
    "print(f\"Confidence threshold set to {model.conf}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcc9ec8b-8492-4252-b992-027ce96e7471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RIYA/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running detection on img2.jpeg ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 183x275 7 persons\n",
      "Speed: 11.4ms pre-process, 217.0ms inference, 18.2ms NMS per image at shape (1, 3, 448, 640)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# # 3. LOAD IMAGE AND RUN DETECTION\n",
    "# =============================================================================\n",
    "# Define the path to your image\n",
    "IMAGE_PATH = 'img2.jpeg' # IMPORTANT: Change this to your image's filename\n",
    "\n",
    "# Load the image using OpenCV\n",
    "try:\n",
    "    img = cv2.imread(IMAGE_PATH)\n",
    "    # YOLO expects images in RGB format, but OpenCV loads them in BGR. We need to convert.\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading image: {e}\")\n",
    "    print(f\"Please make sure '{IMAGE_PATH}' is in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"--- Running detection on {IMAGE_PATH} ---\")\n",
    "# Pass the image to the model to get detections\n",
    "results = model(img_rgb)\n",
    "\n",
    "# The results contain information about bounding boxes, confidence scores, and class labels.\n",
    "results.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e765ba3-03cf-419c-a1fc-51c22587a639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Detected Objects ---\n",
      "         xmin        ymin        xmax        ymax  confidence  class    name\n",
      "0   50.343658   99.007294   60.595615  131.628403    0.773227      0  person\n",
      "1  114.002296  105.395607  126.196594  140.542633    0.654427      0  person\n",
      "2  224.003464   93.580391  233.144501  121.054848    0.621082      0  person\n",
      "3  234.701279   97.121605  242.098648  120.077690    0.617597      0  person\n",
      "4   43.219467   99.361710   49.797241  116.175270    0.574704      0  person\n",
      "5  247.790512   99.314079  258.393768  128.263565    0.483406      0  person\n",
      "6   65.034935  100.868523   70.630127  114.840164    0.410876      0  person\n",
      "\n",
      "--- Bounding boxes and labels drawn on the image ---\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# # 4. PROCESS AND VISUALIZE THE RESULTS\n",
    "# =============================================================================\n",
    "# The esults can be converted to a pandas DataFrame for easier access\n",
    "results_df = results.pandas().xyxy[0]\n",
    "\n",
    "print(\"--- Detected Objects ---\")\n",
    "print(results_df)\n",
    "\n",
    "# Loop through the detected objects and draw boxes on the original image\n",
    "for index, row in results_df.iterrows():\n",
    "    # Get coordinates, class name, and confidence\n",
    "    x1, y1, x2, y2 = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])\n",
    "    confidence = row['confidence']\n",
    "    class_name = row['name']\n",
    "    \n",
    "    # Draw the bounding box\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2) # Blue box with thickness 2\n",
    "    \n",
    "    # Create the label text\n",
    "    label = f'{class_name}: {confidence:.2f}'\n",
    "    \n",
    "    # Put the label text above the box\n",
    "    cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "print(\"\\n--- Bounding boxes and labels drawn on the image ---\")\n",
    "\n",
    "# Display the final image using Matplotlib\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.title('YOLOv5 Object Detection Result', fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.savefig('object_detection_result.png') # Save the final image for LinkedIn\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7318bed0-5f24-4c8a-89f7-f405b07d0b51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
